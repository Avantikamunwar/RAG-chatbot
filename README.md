A lightweight Retrieval-Augmented Generation (RAG) chatbot built using:

FastAPI (Backend API)

Ollama (Local LLM inference)

Nomic Embed Text (Local embedding model)

Pinecone (Vector database)

HTML/JS Frontend (Simple chat UI)

This project allows users to upload documents, build a vector index, and ask questions answered using retrieved context + LLM output.

ðŸš€ Features

âœ… Local LLM using Ollama
âœ… Local embeddings using nomic-embed-text
âœ… Vector search using Pinecone
âœ… Simple RAG pipeline (embed â†’ store â†’ retrieve â†’ generate)
âœ… REST API using FastAPI
âœ… Basic frontend UI for chatting
âœ… Fully offline inference (except Pinecone)